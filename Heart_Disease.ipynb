{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "50128cbe-0034-4bab-b03a-1a7b8e623d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer  \n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4be7acc4-d381-4bd9-a6e3-45be89f37360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in /opt/anaconda3/lib/python3.11/site-packages (0.0.7)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from ucimlrepo) (2.1.4)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /opt/anaconda3/lib/python3.11/site-packages (from ucimlrepo) (2024.6.2)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5234cb6f-e8e5-464f-bb77-b9cd46ea614f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "510ec1d3-c316-456b-9df1-38cb91c555f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 45, 'name': 'Heart Disease', 'repository_url': 'https://archive.ics.uci.edu/dataset/45/heart+disease', 'data_url': 'https://archive.ics.uci.edu/static/public/45/data.csv', 'abstract': '4 databases: Cleveland, Hungary, Switzerland, and the VA Long Beach', 'area': 'Health and Medicine', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 303, 'num_features': 13, 'feature_types': ['Categorical', 'Integer', 'Real'], 'demographics': ['Age', 'Sex'], 'target_col': ['num'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 1989, 'last_updated': 'Fri Nov 03 2023', 'dataset_doi': '10.24432/C52P4X', 'creators': ['Andras Janosi', 'William Steinbrunn', 'Matthias Pfisterer', 'Robert Detrano'], 'intro_paper': {'ID': 231, 'type': 'NATIVE', 'title': 'International application of a new probability algorithm for the diagnosis of coronary artery disease.', 'authors': 'R. Detrano, A. JÃ¡nosi, W. Steinbrunn, M. Pfisterer, J. Schmid, S. Sandhu, K. Guppy, S. Lee, V. Froelicher', 'venue': 'American Journal of Cardiology', 'year': 1989, 'journal': None, 'DOI': None, 'URL': 'https://www.semanticscholar.org/paper/a7d714f8f87bfc41351eb5ae1e5472f0ebbe0574', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': '2756873', 'pmcid': None}, 'additional_info': {'summary': 'This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them.  In particular, the Cleveland database is the only one that has been used by ML researchers to date.  The \"goal\" field refers to the presence of heart disease in the patient.  It is integer valued from 0 (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).  \\n   \\nThe names and social security numbers of the patients were recently removed from the database, replaced with dummy values.\\n\\nOne file has been \"processed\", that one containing the Cleveland database.  All four unprocessed files also exist in this directory.\\n\\nTo see Test Costs (donated by Peter Turney), please see the folder \"Costs\" ', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Only 14 attributes used:\\r\\n      1. #3  (age)       \\r\\n      2. #4  (sex)       \\r\\n      3. #9  (cp)        \\r\\n      4. #10 (trestbps)  \\r\\n      5. #12 (chol)      \\r\\n      6. #16 (fbs)       \\r\\n      7. #19 (restecg)   \\r\\n      8. #32 (thalach)   \\r\\n      9. #38 (exang)     \\r\\n      10. #40 (oldpeak)   \\r\\n      11. #41 (slope)     \\r\\n      12. #44 (ca)        \\r\\n      13. #51 (thal)      \\r\\n      14. #58 (num)       (the predicted attribute)\\r\\n\\r\\nComplete attribute documentation:\\r\\n      1 id: patient identification number\\r\\n      2 ccf: social security number (I replaced this with a dummy value of 0)\\r\\n      3 age: age in years\\r\\n      4 sex: sex (1 = male; 0 = female)\\r\\n      5 painloc: chest pain location (1 = substernal; 0 = otherwise)\\r\\n      6 painexer (1 = provoked by exertion; 0 = otherwise)\\r\\n      7 relrest (1 = relieved after rest; 0 = otherwise)\\r\\n      8 pncaden (sum of 5, 6, and 7)\\r\\n      9 cp: chest pain type\\r\\n        -- Value 1: typical angina\\r\\n        -- Value 2: atypical angina\\r\\n        -- Value 3: non-anginal pain\\r\\n        -- Value 4: asymptomatic\\r\\n     10 trestbps: resting blood pressure (in mm Hg on admission to the hospital)\\r\\n     11 htn\\r\\n     12 chol: serum cholestoral in mg/dl\\r\\n     13 smoke: I believe this is 1 = yes; 0 = no (is or is not a smoker)\\r\\n     14 cigs (cigarettes per day)\\r\\n     15 years (number of years as a smoker)\\r\\n     16 fbs: (fasting blood sugar > 120 mg/dl)  (1 = true; 0 = false)\\r\\n     17 dm (1 = history of diabetes; 0 = no such history)\\r\\n     18 famhist: family history of coronary artery disease (1 = yes; 0 = no)\\r\\n     19 restecg: resting electrocardiographic results\\r\\n        -- Value 0: normal\\r\\n        -- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\\r\\n        -- Value 2: showing probable or definite left ventricular hypertrophy by Estes\\' criteria\\r\\n     20 ekgmo (month of exercise ECG reading)\\r\\n     21 ekgday(day of exercise ECG reading)\\r\\n     22 ekgyr (year of exercise ECG reading)\\r\\n     23 dig (digitalis used furing exercise ECG: 1 = yes; 0 = no)\\r\\n     24 prop (Beta blocker used during exercise ECG: 1 = yes; 0 = no)\\r\\n     25 nitr (nitrates used during exercise ECG: 1 = yes; 0 = no)\\r\\n     26 pro (calcium channel blocker used during exercise ECG: 1 = yes; 0 = no)\\r\\n     27 diuretic (diuretic used used during exercise ECG: 1 = yes; 0 = no)\\r\\n     28 proto: exercise protocol\\r\\n          1 = Bruce     \\r\\n          2 = Kottus\\r\\n          3 = McHenry\\r\\n          4 = fast Balke\\r\\n          5 = Balke\\r\\n          6 = Noughton \\r\\n          7 = bike 150 kpa min/min  (Not sure if \"kpa min/min\" is what was written!)\\r\\n          8 = bike 125 kpa min/min  \\r\\n          9 = bike 100 kpa min/min\\r\\n         10 = bike 75 kpa min/min\\r\\n         11 = bike 50 kpa min/min\\r\\n         12 = arm ergometer\\r\\n     29 thaldur: duration of exercise test in minutes\\r\\n     30 thaltime: time when ST measure depression was noted\\r\\n     31 met: mets achieved\\r\\n     32 thalach: maximum heart rate achieved\\r\\n     33 thalrest: resting heart rate\\r\\n     34 tpeakbps: peak exercise blood pressure (first of 2 parts)\\r\\n     35 tpeakbpd: peak exercise blood pressure (second of 2 parts)\\r\\n     36 dummy\\r\\n     37 trestbpd: resting blood pressure\\r\\n     38 exang: exercise induced angina (1 = yes; 0 = no)\\r\\n     39 xhypo: (1 = yes; 0 = no)\\r\\n     40 oldpeak = ST depression induced by exercise relative to rest\\r\\n     41 slope: the slope of the peak exercise ST segment\\r\\n        -- Value 1: upsloping\\r\\n        -- Value 2: flat\\r\\n        -- Value 3: downsloping\\r\\n     42 rldv5: height at rest\\r\\n     43 rldv5e: height at peak exercise\\r\\n     44 ca: number of major vessels (0-3) colored by flourosopy\\r\\n     45 restckm: irrelevant\\r\\n     46 exerckm: irrelevant\\r\\n     47 restef: rest raidonuclid (sp?) ejection fraction\\r\\n     48 restwm: rest wall (sp?) motion abnormality\\r\\n        0 = none\\r\\n        1 = mild or moderate\\r\\n        2 = moderate or severe\\r\\n        3 = akinesis or dyskmem (sp?)\\r\\n     49 exeref: exercise radinalid (sp?) ejection fraction\\r\\n     50 exerwm: exercise wall (sp?) motion \\r\\n     51 thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\\r\\n     52 thalsev: not used\\r\\n     53 thalpul: not used\\r\\n     54 earlobe: not used\\r\\n     55 cmo: month of cardiac cath (sp?)  (perhaps \"call\")\\r\\n     56 cday: day of cardiac cath (sp?)\\r\\n     57 cyr: year of cardiac cath (sp?)\\r\\n     58 num: diagnosis of heart disease (angiographic disease status)\\r\\n        -- Value 0: < 50% diameter narrowing\\r\\n        -- Value 1: > 50% diameter narrowing\\r\\n        (in any major vessel: attributes 59 through 68 are vessels)\\r\\n     59 lmt\\r\\n     60 ladprox\\r\\n     61 laddist\\r\\n     62 diag\\r\\n     63 cxmain\\r\\n     64 ramus\\r\\n     65 om1\\r\\n     66 om2\\r\\n     67 rcaprox\\r\\n     68 rcadist\\r\\n     69 lvx1: not used\\r\\n     70 lvx2: not used\\r\\n     71 lvx3: not used\\r\\n     72 lvx4: not used\\r\\n     73 lvf: not used\\r\\n     74 cathef: not used\\r\\n     75 junk: not used\\r\\n     76 name: last name of patient  (I replaced this with the dummy string \"name\")', 'citation': None}}\n",
      "        name     role         type demographic  \\\n",
      "0        age  Feature      Integer         Age   \n",
      "1        sex  Feature  Categorical         Sex   \n",
      "2         cp  Feature  Categorical        None   \n",
      "3   trestbps  Feature      Integer        None   \n",
      "4       chol  Feature      Integer        None   \n",
      "5        fbs  Feature  Categorical        None   \n",
      "6    restecg  Feature  Categorical        None   \n",
      "7    thalach  Feature      Integer        None   \n",
      "8      exang  Feature  Categorical        None   \n",
      "9    oldpeak  Feature      Integer        None   \n",
      "10     slope  Feature  Categorical        None   \n",
      "11        ca  Feature      Integer        None   \n",
      "12      thal  Feature  Categorical        None   \n",
      "13       num   Target      Integer        None   \n",
      "\n",
      "                                          description  units missing_values  \n",
      "0                                                None  years             no  \n",
      "1                                                None   None             no  \n",
      "2                                                None   None             no  \n",
      "3   resting blood pressure (on admission to the ho...  mm Hg             no  \n",
      "4                                   serum cholestoral  mg/dl             no  \n",
      "5                     fasting blood sugar > 120 mg/dl   None             no  \n",
      "6                                                None   None             no  \n",
      "7                         maximum heart rate achieved   None             no  \n",
      "8                             exercise induced angina   None             no  \n",
      "9   ST depression induced by exercise relative to ...   None             no  \n",
      "10                                               None   None             no  \n",
      "11  number of major vessels (0-3) colored by flour...   None            yes  \n",
      "12                                               None   None            yes  \n",
      "13                         diagnosis of heart disease   None             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "heart_disease = fetch_ucirepo(id=45) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = heart_disease.data.features \n",
    "y = heart_disease.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(heart_disease.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(heart_disease.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8a62c78d-9401-4163-bef0-d480a3ad5b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   63    1   1       145   233    1        2      150      0      2.3      3   \n",
      "1   67    1   4       160   286    0        2      108      1      1.5      2   \n",
      "2   67    1   4       120   229    0        2      129      1      2.6      2   \n",
      "3   37    1   3       130   250    0        0      187      0      3.5      3   \n",
      "4   41    0   2       130   204    0        2      172      0      1.4      1   \n",
      "\n",
      "    ca  thal  target  \n",
      "0  0.0   6.0       0  \n",
      "1  3.0   3.0       2  \n",
      "2  2.0   7.0       1  \n",
      "3  0.0   3.0       0  \n",
      "4  0.0   3.0       0  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(X, columns=heart_disease.data.feature_names)\n",
    "df['target']=y\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cf04210b-0d07-4858-ab65-a3a10dc51e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after imputation:\n",
      "age         0\n",
      "sex         0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalach     0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          0\n",
      "thal        0\n",
      "target      0\n",
      "dtype: int64\n",
      "Categorical columns: Index([], dtype='object')\n",
      "Training data shape: (242, 13), Testing data shape: (61, 13)\n"
     ]
    }
   ],
   "source": [
    "# 1. Handle missing values (if any)\n",
    "# SimpleImputer with strategy 'mean' to fill missing values for numerical columns\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df.drop(columns='target')), columns=df.drop(columns='target').columns)\n",
    "\n",
    "# Add the target column back\n",
    "df_imputed['target'] = df['target']\n",
    "\n",
    "# Check if there are any missing values left\n",
    "print(f\"Missing values after imputation:\\n{df_imputed.isnull().sum()}\")\n",
    "\n",
    "# 2. Encode categorical variables (if any)\n",
    "# Check for categorical columns\n",
    "categorical_cols = df_imputed.select_dtypes(include=['object']).columns\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# If categorical columns exist, use LabelEncoder or OneHotEncoder\n",
    "# Example: Encoding with LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    df_imputed[col] = label_encoder.fit_transform(df_imputed[col])\n",
    "\n",
    "# 3. Standardize numerical features\n",
    "# Extract the numerical features (excluding the target column)\n",
    "numerical_cols = df_imputed.select_dtypes(include=[np.number]).columns\n",
    "numerical_cols = numerical_cols.drop('target')  # Exclude target\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "df_imputed[numerical_cols] = scaler.fit_transform(df_imputed[numerical_cols])\n",
    "\n",
    "# 4. Split the dataset into training and test sets\n",
    "X = df_imputed.drop(columns='target')\n",
    "y = df_imputed['target']\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shape of training and testing sets\n",
    "print(f\"Training data shape: {X_train.shape}, Testing data shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e10a6ebb-f319-4944-9df4-a060cb805dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after imputation:\n",
      "age         0\n",
      "sex         0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalach     0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          0\n",
      "thal        0\n",
      "target      0\n",
      "dtype: int64\n",
      "\n",
      "Preprocessed data preview:\n",
      "        age       sex        cp  trestbps      chol       fbs   restecg  \\\n",
      "0  0.948726  0.686202 -2.251775  0.757525 -0.264900  2.394438  1.016684   \n",
      "1  1.392002  0.686202  0.877985  1.611220  0.760415 -0.417635  1.016684   \n",
      "2  1.392002  0.686202  0.877985 -0.665300 -0.342283 -0.417635  1.016684   \n",
      "3 -1.932564  0.686202 -0.165268 -0.096170  0.063974 -0.417635 -0.996749   \n",
      "4 -1.489288 -1.457296 -1.208521 -0.096170 -0.825922 -0.417635  1.016684   \n",
      "\n",
      "    thalach     exang   oldpeak     slope        ca      thal  target  \n",
      "0  0.017197 -0.696631  1.087338  2.274579 -0.723095  0.655818       0  \n",
      "1 -1.821905  1.435481  0.397182  0.649113  2.503851 -0.898522       2  \n",
      "2 -0.902354  1.435481  1.346147  0.649113  1.428203  1.173931       1  \n",
      "3  1.637359 -0.696631  2.122573  2.274579 -0.723095 -0.898522       0  \n",
      "4  0.980537 -0.696631  0.310912 -0.976352 -0.723095 -0.898522       0  \n",
      "\n",
      "Statistical Summary of the data:\n",
      "                age           sex            cp      trestbps          chol  \\\n",
      "count  3.030000e+02  3.030000e+02  3.030000e+02  3.030000e+02  3.030000e+02   \n",
      "mean  -1.465641e-18 -2.931282e-17 -1.670831e-16  4.426236e-16  2.345026e-16   \n",
      "std    1.001654e+00  1.001654e+00  1.001654e+00  1.001654e+00  1.001654e+00   \n",
      "min   -2.819115e+00 -1.457296e+00 -2.251775e+00 -2.145037e+00 -2.334877e+00   \n",
      "25%   -7.135564e-01 -1.457296e+00 -1.652679e-01 -6.652997e-01 -6.905030e-01   \n",
      "50%    1.729945e-01  6.862024e-01 -1.652679e-01 -9.616980e-02 -1.101357e-01   \n",
      "75%    7.270888e-01  6.862024e-01  8.779855e-01  4.729601e-01  5.476139e-01   \n",
      "max    2.500191e+00  6.862024e-01  8.779855e-01  3.887739e+00  6.138485e+00   \n",
      "\n",
      "                fbs       restecg       thalach         exang       oldpeak  \\\n",
      "count  3.030000e+02  3.030000e+02  3.030000e+02  3.030000e+02  3.030000e+02   \n",
      "mean  -1.172513e-17 -1.172513e-17 -1.172513e-16 -9.086974e-17  2.345026e-17   \n",
      "std    1.001654e+00  1.001654e+00  1.001654e+00  1.001654e+00  1.001654e+00   \n",
      "min   -4.176345e-01 -9.967493e-01 -3.442067e+00 -6.966305e-01 -8.968617e-01   \n",
      "25%   -4.176345e-01 -9.967493e-01 -7.053073e-01 -6.966305e-01 -8.968617e-01   \n",
      "50%   -4.176345e-01  9.967493e-03  1.485618e-01 -6.966305e-01 -2.067053e-01   \n",
      "75%   -4.176345e-01  1.016684e+00  7.178079e-01  1.435481e+00  4.834512e-01   \n",
      "max    2.394438e+00  1.016684e+00  2.294182e+00  1.435481e+00  4.451851e+00   \n",
      "\n",
      "              slope            ca          thal      target  \n",
      "count  3.030000e+02  3.030000e+02  3.030000e+02  303.000000  \n",
      "mean   1.436328e-16 -2.345026e-17 -4.103795e-17    0.937294  \n",
      "std    1.001654e+00  1.001654e+00  1.001654e+00    1.228536  \n",
      "min   -9.763521e-01 -7.230950e-01 -8.985223e-01    0.000000  \n",
      "25%   -9.763521e-01 -7.230950e-01 -8.985223e-01    0.000000  \n",
      "50%    6.491132e-01 -7.230950e-01 -8.985223e-01    0.000000  \n",
      "75%    6.491132e-01  3.525538e-01  1.173931e+00    2.000000  \n",
      "max    2.274579e+00  2.503851e+00  1.173931e+00    4.000000  \n"
     ]
    }
   ],
   "source": [
    "# Checking for missing values after imputation\n",
    "print(f\"Missing values after imputation:\\n{df_imputed.isnull().sum()}\")\n",
    "\n",
    "# Check the first few rows of the preprocessed data\n",
    "print(f\"\\nPreprocessed data preview:\\n{df_imputed.head()}\")\n",
    "\n",
    "# Check the statistical summary of the dataset\n",
    "print(f\"\\nStatistical Summary of the data:\\n{df_imputed.describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2ed012c0-9f17-49e8-bb71-1a8959c651b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5410\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.90      0.85        29\n",
      "           1       0.38      0.25      0.30        12\n",
      "           2       0.22      0.22      0.22         9\n",
      "           3       0.17      0.29      0.21         7\n",
      "           4       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.54        61\n",
      "   macro avg       0.32      0.33      0.32        61\n",
      "weighted avg       0.51      0.54      0.52        61\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "258427d7-6534-4025-8861-30b4f4b81a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    135\n",
      "1     43\n",
      "3     28\n",
      "2     27\n",
      "4      9\n",
      "Name: count, dtype: int64\n",
      "target\n",
      "0    29\n",
      "1    12\n",
      "2     9\n",
      "3     7\n",
      "4     4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4821aefc-2750-4a2d-978e-63e11cbb1c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.90      0.85        29\n",
      "           1       0.38      0.25      0.30        12\n",
      "           2       0.22      0.22      0.22         9\n",
      "           3       0.17      0.29      0.21         7\n",
      "           4       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.54        61\n",
      "   macro avg       0.32      0.33      0.32        61\n",
      "weighted avg       0.51      0.54      0.52        61\n",
      "\n",
      "[[26  1  2  0  0]\n",
      " [ 3  3  3  3  0]\n",
      " [ 2  1  2  4  0]\n",
      " [ 1  2  2  2  0]\n",
      " [ 0  1  0  3  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "log_reg_preds = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Logistic Regression Evaluation:\")\n",
    "print(classification_report(y_test, log_reg_preds))\n",
    "print(confusion_matrix(y_test, log_reg_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "077c7060-bb5d-4178-9951-57414b60b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a569f6d5-123f-4628-8250-c6fd635f9797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after imputation:\n",
      "age         0\n",
      "sex         0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalach     0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          0\n",
      "thal        0\n",
      "target      0\n",
      "dtype: int64\n",
      "Categorical columns: Index([], dtype='object')\n",
      "Training data shape: (242, 13), Testing data shape: (61, 13)\n",
      "Logistic Regression Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.76      0.81        29\n",
      "           1       0.15      0.17      0.16        12\n",
      "           2       0.14      0.11      0.12         9\n",
      "           3       0.17      0.29      0.21         7\n",
      "           4       0.25      0.25      0.25         4\n",
      "\n",
      "    accuracy                           0.46        61\n",
      "   macro avg       0.32      0.31      0.31        61\n",
      "weighted avg       0.51      0.46      0.48        61\n",
      "\n",
      "[[22  5  2  0  0]\n",
      " [ 3  2  2  4  1]\n",
      " [ 0  3  1  3  2]\n",
      " [ 0  3  2  2  0]\n",
      " [ 0  0  0  3  1]]\n"
     ]
    }
   ],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load data (assuming 'df' is the loaded dataset)\n",
    "# df = pd.read_csv('your_dataset.csv')  # Make sure to load your dataset\n",
    "\n",
    "# 1. Handle missing values (if any)\n",
    "# SimpleImputer with strategy 'mean' to fill missing values for numerical columns\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df.drop(columns='target')), columns=df.drop(columns='target').columns)\n",
    "\n",
    "# Add the target column back\n",
    "df_imputed['target'] = df['target']\n",
    "\n",
    "# Check if there are any missing values left\n",
    "print(f\"Missing values after imputation:\\n{df_imputed.isnull().sum()}\")\n",
    "\n",
    "# 2. Encode categorical variables (if any)\n",
    "# Check for categorical columns\n",
    "categorical_cols = df_imputed.select_dtypes(include=['object']).columns\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# If categorical columns exist, use LabelEncoder or OneHotEncoder\n",
    "# Example: Encoding with LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    df_imputed[col] = label_encoder.fit_transform(df_imputed[col])\n",
    "\n",
    "# 3. Standardize numerical features\n",
    "# Extract the numerical features (excluding the target column)\n",
    "numerical_cols = df_imputed.select_dtypes(include=[np.number]).columns\n",
    "numerical_cols = numerical_cols.drop('target')  # Exclude target\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "df_imputed[numerical_cols] = scaler.fit_transform(df_imputed[numerical_cols])\n",
    "\n",
    "# 4. Split the dataset into training and test sets\n",
    "X = df_imputed.drop(columns='target')\n",
    "y = df_imputed['target']\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shape of training and testing sets\n",
    "print(f\"Training data shape: {X_train.shape}, Testing data shape: {X_test.shape}\")\n",
    "\n",
    "# 5. Logistic Regression Model with class weighting\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "\n",
    "# Train the model\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# 6. Model Evaluation\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Output evaluation metrics\n",
    "print(\"Logistic Regression Evaluation:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "705b9518-ae06-41c1-8902-f6878f8656af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.97      0.84        29\n",
      "           1       0.00      0.00      0.00        12\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.00      0.00      0.00         7\n",
      "           4       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.46        61\n",
      "   macro avg       0.15      0.19      0.17        61\n",
      "weighted avg       0.35      0.46      0.40        61\n",
      "\n",
      "[[28  0  1  0  0]\n",
      " [ 6  0  4  2  0]\n",
      " [ 3  2  0  4  0]\n",
      " [ 1  5  1  0  0]\n",
      " [ 0  2  1  1  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracy scores: [0.60655738 0.60655738 0.55737705 0.56666667 0.55      ]\n",
      "Mean cross-validation accuracy: 0.577431693989071\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize the model\n",
    "rf_clf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "print(\"Random Forest Evaluation:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "# Optional: Cross-validation score for model robustness\n",
    "cv_scores = cross_val_score(rf_clf, X, y, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-validation accuracy scores: {cv_scores}\")\n",
    "print(f\"Mean cross-validation accuracy: {cv_scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2992c33b-9ed5-4697-b322-d9837a56f426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with SMOTE Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.93      0.87        29\n",
      "           1       0.12      0.08      0.10        12\n",
      "           2       0.11      0.11      0.11         9\n",
      "           3       0.20      0.29      0.24         7\n",
      "           4       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.51        61\n",
      "   macro avg       0.25      0.28      0.26        61\n",
      "weighted avg       0.45      0.51      0.48        61\n",
      "\n",
      "[[27  1  1  0  0]\n",
      " [ 5  1  3  3  0]\n",
      " [ 1  3  1  3  1]\n",
      " [ 0  1  4  2  0]\n",
      " [ 0  2  0  2  0]]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE to balance the data\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train the model on the resampled data\n",
    "rf_clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Evaluate the Random Forest model on the test data\n",
    "y_pred_rf_res = rf_clf.predict(X_test)\n",
    "print(\"Random Forest with SMOTE Evaluation:\")\n",
    "print(classification_report(y_test, y_pred_rf_res))\n",
    "print(confusion_matrix(y_test, y_pred_rf_res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7bf9ae4d-e659-402d-996e-5a469e08060e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best cross-validation score: 0.9199999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9854b869-f3d3-4166-accd-b63e2c45586f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with Best Parameters Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.93      0.87        29\n",
      "           1       0.10      0.08      0.09        12\n",
      "           2       0.22      0.22      0.22         9\n",
      "           3       0.25      0.29      0.27         7\n",
      "           4       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.52        61\n",
      "   macro avg       0.28      0.30      0.29        61\n",
      "weighted avg       0.47      0.52      0.50        61\n",
      "\n",
      "[[27  1  1  0  0]\n",
      " [ 5  1  3  2  1]\n",
      " [ 1  4  2  2  0]\n",
      " [ 0  2  3  2  0]\n",
      " [ 0  2  0  2  0]]\n"
     ]
    }
   ],
   "source": [
    "# Using the best parameters\n",
    "rf_clf_best = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=2, min_samples_leaf=1, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_clf_best.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_best = rf_clf_best.predict(X_test)\n",
    "print(\"Random Forest with Best Parameters Evaluation:\")\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "print(confusion_matrix(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6a042ee0-fb94-46bd-ba3e-6aa19eff86ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.86666667 0.91111111 0.91111111 0.97037037 0.94074074]\n",
      "Mean cross-validation accuracy: 0.9199999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(rf_clf_best, X_train_res, y_train_res, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean cross-validation accuracy: {cv_scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a7620e09-f8a7-4c37-bf50-fbdfc1561180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.93      0.87        29\n",
      "           1       0.10      0.08      0.09        12\n",
      "           2       0.22      0.22      0.22         9\n",
      "           3       0.25      0.29      0.27         7\n",
      "           4       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.52        61\n",
      "   macro avg       0.28      0.30      0.29        61\n",
      "weighted avg       0.47      0.52      0.50        61\n",
      "\n",
      "Confusion Matrix:\n",
      " [[27  1  1  0  0]\n",
      " [ 5  1  3  2  1]\n",
      " [ 1  4  2  2  0]\n",
      " [ 0  2  3  2  0]\n",
      " [ 0  2  0  2  0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create the Random Forest model with class_weight='balanced'\n",
    "rf_clf_best_weighted = RandomForestClassifier(n_estimators=200, max_depth=None,\n",
    "                                              min_samples_split=2, min_samples_leaf=1,\n",
    "                                              class_weight='balanced', random_state=42)\n",
    "\n",
    "# Train the model on the resampled data (train set)\n",
    "rf_clf_best_weighted.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_weighted = rf_clf_best_weighted.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_weighted))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_weighted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c9adcf-e9be-4f54-babb-a4384b601d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight': ['balanced', None]  # Include balanced class weight\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Best parameters from GridSearchCV\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Train with the best found parameters\n",
    "best_rf_clf = grid_search.best_estimator_\n",
    "best_rf_clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred_best = best_rf_clf.predict(X_test)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_best))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69484b74-4807-45a9-bd34-4e4b79b781b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
